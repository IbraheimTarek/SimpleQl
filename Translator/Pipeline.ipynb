{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff340bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from farasa.pos import FarasaPOSTagger\n",
    "import torch.nn.init as init\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a8f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataSetClass import Parallel_Data\n",
    "from Preprocessing import Preprocessor\n",
    "from model import Encoder, Decoder, Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9e5da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Parallel_Data(\"./preprocessed_train_data.pkl\",\"./arabic_tokens.json\",\"./english_tokens.json\")\n",
    "val_data = Parallel_Data(\"./preprocessed_val_data.pkl\",\"./arabic_tokens.json\",\"./english_tokens.json\")\n",
    "test_data = Parallel_Data(\"./preprocessed_test_data.pkl\",\"./arabic_tokens.json\",\"./english_tokens.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7834e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(module):\n",
    "    \"\"\"\n",
    "    Custom weight initialization for LSTM, RNN, and Attention/Decoder networks.\n",
    "    Applies Xavier initialization for input-to-hidden weights,\n",
    "    orthogonal initialization for hidden-to-hidden weights,\n",
    "    and sets biases appropriately (including forget-gate bias = 1 for LSTM).\n",
    "    \"\"\"\n",
    "    # LSTM layers\n",
    "    if isinstance(module, nn.LSTM):\n",
    "        for name, param in module.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "                # Set forget-gate bias to 1\n",
    "                n = param.size(0)\n",
    "                start, end = n // 4, n // 2\n",
    "                param.data[start:end].fill_(1)\n",
    "\n",
    "    # Vanilla RNN/GRU layers\n",
    "    elif isinstance(module, (nn.RNN, nn.GRU)):\n",
    "        for name, param in module.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "\n",
    "    # Linear layers (used in attention and decoder output)\n",
    "    elif isinstance(module, nn.Linear):\n",
    "        init.xavier_uniform_(module.weight.data)\n",
    "        if module.bias is not None:\n",
    "            module.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289ee2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim_arabic = len(train_data.arabic_tokens)\n",
    "input_dim_postag = len(train_data.postags)\n",
    "OUTPUT_DIM = len(train_data.english_tokens)\n",
    "\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "\n",
    "HID_DIM = 256\n",
    "N_LAYERS = 1\n",
    "\n",
    "train_dataloader = DataLoader(train_data,32,shuffle=True)\n",
    "val_dataloader = DataLoader(val_data,256,shuffle=False)\n",
    "test_dataloader = DataLoader(test_data,256,shuffle=False)\n",
    "\n",
    "enc_arabic = Encoder(input_dim_arabic, ENC_EMB_DIM, HID_DIM, N_LAYERS,device)\n",
    "enc_postag = Encoder(input_dim_postag, ENC_EMB_DIM, HID_DIM, N_LAYERS,device)\n",
    "\n",
    "dec = Decoder(\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    emb_dim=DEC_EMB_DIM,\n",
    "    hid_dim=HID_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    enc_hid_dim=HID_DIM * 4,  # 2 encoders * bidirectional\n",
    "    attn_dim=128\n",
    ")\n",
    "\n",
    "enc_arabic.apply(init_weights)\n",
    "enc_postag.apply(init_weights)\n",
    "dec.apply(init_weights)\n",
    "\n",
    "model = Seq2Seq(enc_arabic, enc_postag, dec, device).to(device)\n",
    "\n",
    "# Initialize token frequency counts\n",
    "token_counts = torch.zeros(len(train_data.english_tokens), dtype=torch.long)\n",
    "\n",
    "# Accumulate token frequencies from target batches\n",
    "for _,trg_batch,_,_ in train_dataloader:\n",
    "    trg = trg_batch.to(\"cpu\")  # Ensure on CPU for bincount\n",
    "    token_counts += torch.bincount(\n",
    "        trg.flatten(), minlength=len(train_data.english_tokens)\n",
    "    )\n",
    "\n",
    "# Avoid division by zero for padding (index 0)\n",
    "token_counts[0] = 0\n",
    "\n",
    "# Compute inverse square root frequency weights\n",
    "weights = 1.0 / torch.sqrt(token_counts.float() + 1e-5)\n",
    "weights[0] = 0  # Padding should not contribute to the loss\n",
    "# UNK token weight\n",
    "weights[-1] = 0\n",
    "# Normalize weights to keep loss scale reasonable\n",
    "weights = weights / weights.mean()\n",
    "\n",
    "# Define loss function with weighting\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    weight=weights.to(device),\n",
    "    ignore_index=0,       # Padding index\n",
    "    label_smoothing=0.1\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode= \"max\",factor=0.5,patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829da3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def plot_attention(attention, source_tokens, target_tokens, epoch=None, filename=None):\n",
    "    \"\"\"\n",
    "    Plot attention heatmap\n",
    "    attention: numpy array of shape [target_len, source_len]\n",
    "    source_tokens: list of source tokens\n",
    "    target_tokens: list of target tokens\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    cax = ax.matshow(attention, cmap='viridis')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticks(np.arange(len(source_tokens)))\n",
    "    ax.set_yticks(np.arange(len(target_tokens)))\n",
    "    ax.set_xticklabels(source_tokens, rotation=90)\n",
    "    ax.set_yticklabels(target_tokens)\n",
    "\n",
    "    # Force label every token\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.title(f\"Attention Weights (Epoch {epoch})\" if epoch else \"Attention Weights\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if filename:\n",
    "        plt.savefig(filename, bbox_inches='tight')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08580d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, clip):\n",
    "    model.train()  \n",
    "    epoch_loss = 0\n",
    "    for src, trg, src_length , postags in dataloader:\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        postags = postags.to(device)\n",
    "        src_length = src_length.to(device)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        \n",
    "        output, _ = model(src, trg,src_length, postags, teacher_forcing_ratio = 0.5)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()  \n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    print(\"done\")\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion,epoch, return_attention ,sample_index=0):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            src, trg, src_len, postags = batch\n",
    "            src_len = src_len.to(device)\n",
    "\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            postags = postags.to(device)\n",
    "    \n",
    "            output, attentions  = model(src, trg, src_len, postags, teacher_forcing_ratio = 0,return_attentions = return_attention)\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            trg_flat = trg[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg_flat)\n",
    "            epoch_loss += loss.item()\n",
    "              \n",
    "    # Capture attention for the first sample in the last batch\n",
    "    if i == len(dataloader) - 1 and attentions:\n",
    "        # Convert to numpy and select sample\n",
    "        attn_matrix = torch.stack(attentions).squeeze(1)[:, sample_index, :]\n",
    "        sample_attentions = attn_matrix.numpy()\n",
    "             \n",
    "    inv_src_vocab = {i: w for w, i in train_data.arabic_tokens.items() }\n",
    "    inv_trg_vocab = {i: w for w, i in train_data.english_tokens.items() }\n",
    "    # Prepare tokens for visualization\n",
    "    src_tokens = [inv_src_vocab[idx] for idx in src[sample_index].cpu().numpy() \n",
    "                 if idx not in [0, train_data.arabic_tokens[\"<s>\"], train_data.arabic_tokens[\"</s>\"]]]  # exclude padding, <sos>, <eos>\n",
    "\n",
    "    trg_tokens = [inv_trg_vocab[idx] for idx in trg[sample_index].cpu().numpy() \n",
    "                 if idx not in [0, train_data.english_tokens[\"<s>\"], train_data.english_tokens[\"</s>\"]]]\n",
    "    \n",
    "    # Visualize if we have attention\n",
    "    if sample_attentions is not None:\n",
    "        plot_filename = f\"attention_epoch_{epoch}.png\"\n",
    "        plot_attention(\n",
    "            sample_attentions, \n",
    "            source_tokens=src_tokens,\n",
    "            target_tokens=trg_tokens,\n",
    "            epoch=epoch,\n",
    "            filename=plot_filename\n",
    "        )\n",
    "    model.train()\n",
    "\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3225d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(40), desc=\"Epochs\"):\n",
    "\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion, clip=1)\n",
    "    val_loss = evaluate(model, val_dataloader,criterion, epoch ,True)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1:02}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "    bleu_score = compute_dataset_bleu(model,val_dataloader,train_data.english_tokens,device,val_data.max_length_english)\n",
    "    print(bleu_score)\n",
    "    # scheduler.step(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "import torch\n",
    "\n",
    "# Ensure required NLTK data is downloaded\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "smooth_fn = SmoothingFunction().method4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a62fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model,src, src_length, postags, max_length, sos_index, eos_index):\n",
    "        batch_size = src.size(0)\n",
    "        device = src.device\n",
    "        \n",
    "        # Encode source sequences\n",
    "        enc_outs_arabic, hidden_arabic, cell_arabic = model.encoder_arabic(src, src_length)\n",
    "        enc_outs_postag, hidden_postag, cell_postag = model.encoder_postag(postags, src_length)\n",
    "        \n",
    "        # Combine encoder outputs\n",
    "        combined_enc_outs = torch.cat((enc_outs_arabic, enc_outs_postag), dim=2)\n",
    "        max_src_len = combined_enc_outs.size(1)\n",
    "        \n",
    "        # Create mask from source lengths\n",
    "        mask = model.create_mask(src_length, max_src_len)\n",
    "        \n",
    "        # Initialize decoder states\n",
    "        hidden = model.enc2dec(torch.cat((hidden_arabic, hidden_postag), dim=2))\n",
    "        cell = model.enc2dec(torch.cat((cell_arabic, cell_postag), dim=2))\n",
    "        \n",
    "        # Initialize output tensor with SOS tokens\n",
    "        output_ids = torch.full((batch_size, max_length), eos_index, dtype=torch.long, device=device)\n",
    "        output_ids[:, 0] = sos_index\n",
    "        \n",
    "        # Track finished sequences\n",
    "        unfinished = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
    "        \n",
    "        # Track which sequences are active in current step\n",
    "        active_mask = torch.arange(batch_size, device=device)\n",
    "        \n",
    "        # Autoregressive generation\n",
    "        for t in range(1, max_length):\n",
    "            # Get last predicted tokens for active sequences\n",
    "            input = output_ids[active_mask, t-1]  # [current_batch_size]\n",
    "            \n",
    "            # Run decoder for active sequences\n",
    "            decoder_output, hidden_step, cell_step, _ = model.decoder(\n",
    "                input=input,\n",
    "                hidden=hidden[:, active_mask, :],\n",
    "                cell=cell[:, active_mask, :],\n",
    "                encoder_outputs=combined_enc_outs[active_mask],\n",
    "                mask=mask[active_mask]\n",
    "            )\n",
    "            \n",
    "            # Greedy token selection\n",
    "            next_tokens = decoder_output.argmax(dim=-1)\n",
    "            output_ids[active_mask, t] = next_tokens\n",
    "            \n",
    "            # Update states for active sequences\n",
    "            hidden[:, active_mask, :] = hidden_step\n",
    "            cell[:, active_mask, :] = cell_step\n",
    "            \n",
    "            # Update which sequences are still active\n",
    "            unfinished[active_mask] = (next_tokens != eos_index)\n",
    "            active_mask = torch.nonzero(unfinished, as_tuple=False).squeeze(-1)\n",
    "            \n",
    "            # Early termination if no active sequences\n",
    "            if active_mask.nelement() == 0:\n",
    "                break\n",
    "        \n",
    "        return output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a2c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dataset_bleu(model, dataloader, english_tokens, device, max_length=50, weights=(0.25, 0.25, 0.25, 0.25)):\n",
    "    \"\"\"\n",
    "    Run the model over the dataloader to compute BLEU.\n",
    "\n",
    "    Args:\n",
    "        model: Seq2Seq model with .generate method for inference.\n",
    "        dataloader: yields tuples (src, trg, src_len, postags).\n",
    "        english_tokens: Dictionary mapping tokens to IDs for target language.\n",
    "        device: torch device.\n",
    "        max_length: maximum generation length.\n",
    "    Returns:\n",
    "        float: corpus-level BLEU score.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    references = []  # List of reference sentences (each wrapped in a list)\n",
    "    hypotheses = []  # List of hypothesis sentences\n",
    "\n",
    "    # Get special token IDs\n",
    "    sos_id = english_tokens.get(\"<s>\")\n",
    "    eos_id = english_tokens.get(\"</s>\")\n",
    "    unk_id = english_tokens.get(\"<unk>\")\n",
    "    \n",
    "    # Create inverse vocabulary for decoding (with fallback for unknown tokens)\n",
    "    inv_trg_vocab = {idx: token for token, idx in english_tokens.items()}\n",
    "    \n",
    "    # Smoothing function for BLEU\n",
    "    smooth_fn = SmoothingFunction().method1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, trg, src_len, postags in dataloader:\n",
    "            src, src_len = src.to(device), src_len.to(device)\n",
    "            postags = postags.to(device)\n",
    "            \n",
    "            # Generate sequences using model's inference method\n",
    "            pred_ids = generate(\n",
    "                model,\n",
    "                src=src, \n",
    "                src_length=src_len, \n",
    "                postags=postags,\n",
    "                max_length=max_length,\n",
    "                sos_index=sos_id,\n",
    "                eos_index=eos_id\n",
    "            )  # Shape: [batch_size, max_seq_len]\n",
    "\n",
    "            # Process each example in the batch\n",
    "            for i in range(pred_ids.size(0)):\n",
    "                # Remove SOS and get tokens until EOS for reference\n",
    "                ref_raw = trg[i].tolist()\n",
    "                ref_tokens = []\n",
    "                for tok_id in ref_raw:\n",
    "                    if tok_id == sos_id:\n",
    "                        continue\n",
    "                    if tok_id == eos_id:\n",
    "                        break\n",
    "                    ref_tokens.append(tok_id)\n",
    "                \n",
    "                # Remove EOS and beyond for hypothesis\n",
    "                hyp_raw = pred_ids[i].tolist()\n",
    "                hyp_tokens = []\n",
    "                for tok_id in hyp_raw:\n",
    "                    if tok_id == eos_id:\n",
    "                        break\n",
    "                    hyp_tokens.append(tok_id)\n",
    "                \n",
    "                # Convert token IDs to words\n",
    "                ref_words = [inv_trg_vocab.get(idx, \"<unk>\") for idx in ref_tokens]\n",
    "                hyp_words = [inv_trg_vocab.get(idx, \"<unk>\") for idx in hyp_tokens]\n",
    "                \n",
    "                references.append([ref_words])  # Wrap in list for corpus_bleu\n",
    "                hypotheses.append(hyp_words)\n",
    "\n",
    "    # Compute corpus-level BLEU\n",
    "    return corpus_bleu(\n",
    "        list_of_references=references,\n",
    "        hypotheses=hypotheses,\n",
    "        weights=weights,\n",
    "        smoothing_function=smooth_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fd9e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_vocab, postag_vocab, trg_vocab, model, device, max_len=50):\n",
    "    # Tokenize and POS tag the sentence\n",
    "    postagger = FarasaPOSTagger()\n",
    "    sequence = postagger.tag_segments(sentence)\n",
    "    tokens = [item.tokens[0] for item in sequence]\n",
    "    tags = [item.tags[0] for item in sequence]\n",
    "\n",
    "    # Numericalize tokens and tags\n",
    "    numericalized_tokens = (\n",
    "        [src_vocab[\"<s>\"]]\n",
    "        + [src_vocab.get(token, src_vocab[\"<UNK>\"]) for token in tokens]\n",
    "        + [src_vocab[\"</s>\"]]\n",
    "    )\n",
    "    numericalized_tags = (\n",
    "        [postag_vocab[\"<s>\"]]\n",
    "        + [postag_vocab.get(tag, postag_vocab[\"<UNK>\"]) for tag in tags]\n",
    "        + [postag_vocab[\"</s>\"]]\n",
    "    )\n",
    "    \n",
    "    # Convert to tensors\n",
    "    tensor_tokens = torch.tensor(numericalized_tokens).unsqueeze(0).to(device)  # [1, seq_len]\n",
    "    tensor_tags = torch.tensor(numericalized_tags).unsqueeze(0).to(device)      # [1, seq_len]\n",
    "    src_len = torch.tensor([len(numericalized_tokens)]).to(device)\n",
    "    \n",
    "    # Create mask (all True since it's a single non-padded sequence)\n",
    "    mask = torch.ones(1, len(numericalized_tokens), dtype=torch.bool).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get encoder outputs\n",
    "        enc_outs_arabic, hidden_arabic, cell_arabic = model.encoder_arabic(tensor_tokens, src_len)\n",
    "        enc_outs_postag, hidden_postag, cell_postag = model.encoder_postag(tensor_tags, src_len)\n",
    "        \n",
    "        # Combine encoder outputs\n",
    "        combined_enc_outs = torch.cat((enc_outs_arabic, enc_outs_postag), dim=2)\n",
    "        \n",
    "        # Combine and project hidden states\n",
    "        hidden = model.enc2dec(torch.cat((hidden_arabic, hidden_postag), dim=2))\n",
    "        cell = model.enc2dec(torch.cat((cell_arabic, cell_postag), dim=2))\n",
    "    \n",
    "    # Initialize with SOS token\n",
    "    trg_indexes = [trg_vocab[\"<s>\"]]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Use the updated decoder interface\n",
    "            output, hidden, cell, _ = model.decoder(\n",
    "                trg_tensor, \n",
    "                hidden, \n",
    "                cell,\n",
    "                combined_enc_outs,\n",
    "                mask\n",
    "            )\n",
    "        \n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "        \n",
    "        # Stop if EOS is generated\n",
    "        if pred_token == trg_vocab[\"</s>\"]:\n",
    "            break\n",
    "    \n",
    "\n",
    "    return trg_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be64de",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = translate_sentence(\"من هم اصدقاء بوب ؟\",train_data.arabic_tokens,train_data.postags\\\n",
    "                         ,train_data.english_tokens,model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feeefd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_trg_vocab = {i: w for w, i in train_data.english_tokens.items() }\n",
    "translated_tokens = [inv_trg_vocab[idx] for idx in out]\n",
    "translated_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
