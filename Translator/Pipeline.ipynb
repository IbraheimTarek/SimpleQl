{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff340bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from farasa.pos import FarasaPOSTagger\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6a8f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataSetClass import Parallel_Data\n",
    "from Preprocessing import Preprocessor\n",
    "from model import Encoder, Decoder, Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be9e5da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Parallel_Data(\"./preprocessed_train_data.pkl\",\"./arabic_tokens.json\",\"./english_tokens.json\")\n",
    "val_data = Parallel_Data(\"./preprocessed_val_data.pkl\",\"./arabic_tokens.json\",\"./english_tokens.json\")\n",
    "test_data = Parallel_Data(\"./preprocessed_test_data.pkl\",\"./arabic_tokens.json\",\"./english_tokens.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "289ee2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim_arabic = len(train_data.arabic_tokens)\n",
    "input_dim_postag = len(train_data.postags)\n",
    "OUTPUT_DIM = len(train_data.english_tokens)\n",
    "ENC_EMB_DIM = 100\n",
    "DEC_EMB_DIM = 100\n",
    "HID_DIM = 1024\n",
    "N_LAYERS = 2\n",
    "train_dataloader = DataLoader(train_data,16,shuffle=True)\n",
    "val_dataloader = DataLoader(val_data,256,shuffle=True)\n",
    "#test_dataloader = DataLoader(train_data,256,shuffle=True)\n",
    "\n",
    "enc_arabic = Encoder(input_dim_arabic, ENC_EMB_DIM, HID_DIM, N_LAYERS,device)\n",
    "enc_postag = Encoder(input_dim_postag, ENC_EMB_DIM, HID_DIM, N_LAYERS,device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS)\n",
    "model = Seq2Seq(enc_arabic, enc_postag, dec, device).to(device)\n",
    "\n",
    "# Initialize token frequency counts\n",
    "token_counts = torch.zeros(len(train_data.english_tokens), dtype=torch.long)\n",
    "\n",
    "# Accumulate token frequencies from target batches\n",
    "for _,trg_batch,_,_ in train_dataloader:\n",
    "    trg = trg_batch.to(\"cpu\")  # Ensure on CPU for bincount\n",
    "    token_counts += torch.bincount(\n",
    "        trg.flatten(), minlength=len(train_data.english_tokens)\n",
    "    )\n",
    "\n",
    "# Avoid division by zero for padding (index 0)\n",
    "token_counts[0] = 0\n",
    "\n",
    "# Compute inverse square root frequency weights\n",
    "weights = 1.0 / torch.sqrt(token_counts.float() + 1e-5)\n",
    "weights[0] = 0  # Padding should not contribute to the loss\n",
    "# UNK token weight\n",
    "weights[-1] = 0\n",
    "# Normalize weights to keep loss scale reasonable\n",
    "weights = weights / weights.mean()\n",
    "\n",
    "# Define loss function with weighting\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    weight=weights.to(device),\n",
    "    ignore_index=0,       # Padding index\n",
    "    label_smoothing=0.1\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08580d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, clip):\n",
    "    model.train()  \n",
    "    epoch_loss = 0\n",
    "    for src, trg, src_length , postags in dataloader:\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        postags = postags.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg,src_length, postags, teacher_forcing_ratio = 0.6)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()  \n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg, src_len, postags in dataloader:\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            postags = postags.to(device)\n",
    "\n",
    "            output = model(src, trg, src_len, postags, 0)\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    model.train()\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3225d080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(40), desc=\"Epochs\"):\n",
    "\n",
    "    train_loss = train(model, val_dataloader, optimizer, criterion, clip=1)\n",
    "    val_loss = evaluate(model, val_dataloader, criterion)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1:02}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8fd9e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_vocab, postag_vocab, trg_vocab, model, device, max_len=50):\n",
    "    postagger = FarasaPOSTagger()\n",
    "    sequence = postagger.tag_segments(sentence)\n",
    "    tokens = [item.tokens[0] for item in sequence]\n",
    "    tags = [item.tags[0] for item in sequence ]\n",
    "\n",
    "    numericalized_tokens = (\n",
    "        [src_vocab[\"<s>\"]]\n",
    "        + [src_vocab.get(token, src_vocab[\"<UNK>\"]) for token in tokens]\n",
    "        + [src_vocab[\"</s>\"]]\n",
    "    )\n",
    "    numericalized_tags = (\n",
    "        [postag_vocab[\"<s>\"]]\n",
    "        + [src_vocab.get(tag, postag_vocab[\"<UNK>\"]) for tag in tags]\n",
    "        + [postag_vocab[\"</s>\"]]\n",
    "    )\n",
    "    tensor_tokens = torch.tensor(numericalized_tokens).unsqueeze(0).to(device)  # shape: [1, seq_len]\n",
    "    tensor_tags   = torch.tensor(numericalized_tags).unsqueeze(0).to(device)  # shape: [1, seq_len]\n",
    "\n",
    "    srclen = torch.tensor([len(numericalized_tokens)],dtype=torch.int64).to(device)\n",
    "\n",
    "    # Encode the source sentence\n",
    "    with torch.no_grad():\n",
    "        hidden_tokens, cell_tokens = model.encoder_arabic(tensor_tokens,srclen)\n",
    "        hidden_tags, cell_tags = model.encoder_postag(tensor_tags,srclen)\n",
    "        hidden = model.enc2dec(torch.cat((hidden_tokens, hidden_tags), dim=2))\n",
    "        cell = model.enc2dec(torch.cat((cell_tokens, cell_tags), dim=2))\n",
    "        \n",
    "    # Initialize the decoder input with <SOS>\n",
    "    trg_indexes = [trg_vocab[\"<s>\"]]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
    "\n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "        if pred_token == trg_vocab[\"</s>\"]:\n",
    "            break\n",
    "\n",
    "    return trg_indexes[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79be64de",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = translate_sentence(\"ما هو متوسط ساعات اليوم ؟\",train_data.arabic_tokens,train_data.postags\\\n",
    "                         ,train_data.english_tokens,model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4feeefd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'are',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_trg_vocab = {i: w for w, i in train_data.english_tokens.items() }\n",
    "translated_tokens = [inv_trg_vocab[idx] for idx in out]\n",
    "translated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20359486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
